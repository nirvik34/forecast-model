# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u0DT6YrrhrdKUDXU1sra4O9T3Ychd7sa
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

df = pd.read_csv('/content/sample_data/2025-grievances.csv')

# print(df.head())
# print(df.columns)

df['Grievance Date'] = pd.to_datetime(df['Grievance Date'])

#Filter specific grievance

mask = df['Sub Category'] == 'Street Light Not Working'
df_sub = df[mask]

daily = df_sub.groupby(df_sub["Grievance Date"].dt.date).size().reset_index(name = 'count')
daily['date'] = pd.to_datetime(daily['Grievance Date'])
daily = daily[['date','count']]

full_range = pd.date_range(daily['date'].min(),daily['date'].max(),freq = 'D')

daily = daily.set_index('date').reindex(full_range, fill_value=0).rename_axis('date').reset_index()

# print(daily.head(10))

"""Add Seasonality Feature"""

daily['month'] = daily['date'].dt.month

# def get_season(month):
#     if month in [6]:
#       return 'Monsoon'
#     elif month in [3,4,5]:
#       return 'Summer'
#     elif month in [11,12,1,2]:
#       return 'Winter'
#     else: return 'Post-monsoon''''


daily['month_sin'] = np.sin(2* np.pi * daily['month']/12)
daily['month_cos'] = np.cos(2* np.pi * daily['month']/12)

"""Festival data"""

festival_dates = [
    '2025-03-29',  # e.g., Ugadi
    '2025-04-12',  # e.g., Ram Navami
    '2025-05-16',  # Add more as needed
]
daily['is_festival'] = daily['date'].astype(str).isin(festival_dates).astype(int)
daily['is_festival_shift-1'] = daily['is_festival'].shift(1).fillna(0)
daily['is_festival_shift+1'] = daily['is_festival'].shift(-1).fillna(0)
print(daily.head(10))

"""LSTM DATA PROCESSING

select features for model
"""

features = ['count', 'month_sin', 'month_cos', 'is_festival', 'is_festival_shift-1', 'is_festival_shift+1']
X_all = daily[features].values.astype(float)

sequence_length = 30
def create_sequences(data, seq_len):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len][0])   # Predict 'count' for next day after window
    return np.array(X), np.array(y)

X_all_seq, y_all = create_sequences(X_all, sequence_length)
print("Shape after windowing:", X_all_seq.shape, y_all.shape)

split = int(0.8 * len(X_all_seq))
X_train, X_test = X_all_seq[:split], X_all_seq[split:]
y_train, y_test = y_all[:split], y_all[split:]

from sklearn.preprocessing import MinMaxScaler

# Reshape for scaler: (samples * timesteps, features)
X_train_2d = X_train.reshape(-1, X_train.shape[2])
X_test_2d = X_test.reshape(-1, X_test.shape[2])

feature_scaler = MinMaxScaler()
X_train_scaled_2d = feature_scaler.fit_transform(X_train_2d)
X_test_scaled_2d = feature_scaler.transform(X_test_2d)

X_train_scaled = X_train_scaled_2d.reshape(X_train.shape)
X_test_scaled = X_test_scaled_2d.reshape(X_test.shape)

count_scaler = MinMaxScaler()
y_train_scaled = count_scaler.fit_transform(y_train.reshape(-1,1)).flatten()
y_test_scaled = count_scaler.transform(y_test.reshape(-1,1)).flatten()

"""Split the data and Build the LSTM Model"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Activation

"""Train/test"""

# model = Sequential([
#     Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
#     MaxPooling1D(pool_size=2),
#     LSTM(64, return_sequences=True),
#     Dropout(0.3),
#     LSTM(32),
#     Dense(1)
# ])

model = Sequential([
    LSTM(32, return_sequences=True, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])),
    LSTM(16),
    Dense(1),
    Activation('sigmoid')
])
model.compile(optimizer='adam', loss='mse')

# model.compile(optimizer=Adam(learning_rate=0.0005), loss=tf.keras.losses.Huber())

# es = EarlyStopping(patience=5, restore_best_weights=True)

#  foe large models
# history = model.fit(
#     X_train, y_train,
#     epochs=100,
#     batch_size=16,
#     validation_data=(X_test, y_test),
#     callbacks=[es],
#     verbose=1
# )

#for small models
es = EarlyStopping(patience=7, restore_best_weights=True)
history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=4,  # smaller batch for small dataset
    validation_data=(X_test, y_test),
    callbacks=[es],
    verbose=1
)

"""Evaluate and Visualize Predictions"""

y_pred_scaled = model.predict(X_test_scaled)

# Inverse transform using count_scaler
y_pred_inv = count_scaler.inverse_transform(y_pred_scaled)
y_test_inv = count_scaler.inverse_transform(y_test.reshape(-1,1))

y_pred_scaled = model.predict(X_test_scaled)
print(y_pred_scaled[:10])

print('Train target min/max:', y_train.min(), y_train.max())
print('Test target min/max:', y_test.min(), y_test.max())
print('Scaled train target min/max:', y_train_scaled.min(), y_train_scaled.max())
print('Scaled test target min/max:', y_test_scaled.min(), y_test_scaled.max())
print('y_pred_scaled:', y_pred_scaled[:10].flatten())
print('y_pred_inv:', y_pred_inv[:10].flatten())
print('y_test_inv:', y_test_inv[:10].flatten())

print("y_pred_inv min/max:", y_pred_inv.min(), y_pred_inv.max())
print("y_test_inv min/max:", y_test_inv.min(), y_test_inv.max())

print("y_pred_scaled:", y_pred_scaled[:10].flatten())
print("y_pred_inv:", y_pred_inv[:10].flatten())
print("y_test_inv:", y_test_inv[:10].flatten())

plt.figure(figsize=(12,5))
plt.plot(y_test_inv, label='Actual', linewidth=2)
plt.plot(y_pred_inv, label='Predicted', linestyle='dashed')
plt.legend()
plt.title('LSTM Forecast (with Seasonality + Festivals)')
plt.xlabel('Test Sample Index')
plt.ylabel('Daily Complaints')
plt.grid(True)
plt.show()

plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='val')
plt.legend()
plt.show()